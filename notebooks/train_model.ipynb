{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MNIST Model - Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train is the dataset of 28x28 images of handwritten digits that the model will be trained on.\n",
    "\n",
    "y_train is the dataset of labels that correspond to x_train.\n",
    "\n",
    "x_test is the dataset of 28x28 images of handwritten digits that the model will be tested on.\n",
    "\n",
    "y_test is the dataset of labels that correspond to x_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Reshaping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first convolution layer expects a single 60000x28x28x1 tensor instead of 60000 28x28x1 tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our image shape is 28X28\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# reshape train\n",
    "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "# reshape test\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "# normalize values\n",
    "x_train=x_train / 255.0\n",
    "x_test=x_test/255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels for the training and the testing dataset are currently categorical and is not continuous. To include categorical dataset in our model, our labels should be converted to one-hot encodings.\n",
    "\n",
    "For example, 2 becomes [0,0,1,0,0,0,0,0,0,0] and 7 becomes [0,0,0,0,0,0,0,1,0,0].\n",
    "\n",
    "Run the following cell to transform the labels into one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
    "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets build out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure description\n",
    "\n",
    "- Conv2D layers are convolutions. Each filter (32 in the first two convolution layers and 64 in the next two convolution layers) transforms a part of the image (5x5 for the first two Conv2D layers and 3x3 for the next two Conv2D layers). The transformation is applied on the whole image.\n",
    "\n",
    "- MaxPool2D is a downsampling filter. It reduces a 2x2 matrix of the image to a single pixel with the maximum value of the 2x2 matrix. The filter aims to conserve the main features of the image while reducing the size.\n",
    "\n",
    "- Dropout is a regularization layer. In our model, 25% of the nodes in the layer are randomly ignores, allowing the network to learn different features. This prevents overfitting.\n",
    "\n",
    "- relu is the rectifier, and it is used to find nonlinearity in the data. It works by returning the input value if the input value >= 0. If the input is negative, it returns 0.\n",
    "\n",
    "- Flatten converts the tensors into a 1D vector.\n",
    "\n",
    "- The Dense layers are an artificial neural network (ANN). The last layer returns the probability that an image is in each class (one for each digit).\n",
    "\n",
    "As this model aims to categorize the images, we will use a categorical_crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
